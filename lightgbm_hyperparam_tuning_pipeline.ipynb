{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class ModelPipeline(object):\n",
    "    \n",
    "    __metaclass__ = abc.ABCMeta\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_data(self, *args, **kwargs):\n",
    "        pass    \n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def fit_model(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    # Hook method\n",
    "    def optimize_hyperparam(self, *args, **kwargs):\n",
    "        pass    \n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def transform_model(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def ensemble_model(self, *args, **kwargs):\n",
    "        pass    \n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def evaluate_result(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    #定義method的執行流程\n",
    "    def execute_pipeline(self, data, label, train_test_col):\n",
    "        self.get_data(data, label, train_test_col)\n",
    "        self.optimize_hyperparam()\n",
    "        self.fit_model()\n",
    "        self.transform_model()\n",
    "        self.evaluate_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "MAX_EVALS = 500\n",
    "N_FOLDS = 5\n",
    "# lgb hyperparameter grid\n",
    "PARAM_GRID = {\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(30, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log(0.005), np.log(0.2), base = np.exp(1), num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10))\n",
    "}\n",
    "# Subsampling (only applicable with 'goss')\n",
    "SUBSAMPLE_DIST = list(np.linspace(0.5, 1, 100))\n",
    "\n",
    "\n",
    "class TrainLGBModel(ModelPipeline):\n",
    "    def __init__(self, search_type):\n",
    "        self.search_type = search_type\n",
    "        self._model = None\n",
    "        self._fitted_model = None\n",
    "        self._prediction = None\n",
    "        self._features = None\n",
    "        self.labels = None\n",
    "        self.test_labels = None\n",
    "        self._test_features = None\n",
    "        self._train_set = None\n",
    "    \n",
    "    def get_data(self, data, label, train_test_col):\n",
    "        data = pd.read_csv(data)\n",
    "        train = data[data[train_test_col] == 'train']\n",
    "        test = data[data[train_test_col] == 'test']\n",
    "        \n",
    "        train_labels = np.array(train[label].astype(np.int32)).reshape((-1,))\n",
    "        self.test_labels = np.array(test[label].astype(np.int32)).reshape((-1,))\n",
    "        \n",
    "        train = train.drop(columns = [train_test_col, label])\n",
    "        test = test.drop(columns = [train_test_col, label])\n",
    "        self._features = np.array(train)\n",
    "        self._test_features = np.array(test)\n",
    "        self.labels = train_labels[:] \n",
    "        print('Train shape: ', train.shape)\n",
    "        print('Test shape: ', test.shape)\n",
    "        print(\"Successfully load training data!\")\n",
    "        self._train_set = lgb.Dataset(self._features, label = self.labels)\n",
    "#         return self._features, self._test_features, self._train_set\n",
    "    \n",
    "    def optimize_hyperparam(self):\n",
    "        if self.search_type == 'default':\n",
    "            self._model = lgb.LGBMClassifier()\n",
    "        if self.search_type == 'bayesian':\n",
    "            pass\n",
    "        if self.search_type == 'random':\n",
    "            #random.seed(100)\n",
    "            for i in range(MAX_EVALS):\n",
    "                # Randomly sample parameters for gbm\n",
    "                params = {key: random.sample(value, 1)[0] for key, value in PARAM_GRID.items()}            \n",
    "                if params['boosting_type'] == 'goss':\n",
    "                    # Cannot subsample with goss\n",
    "                    params['subsample'] = 1.0                    \n",
    "                else:\n",
    "                    # Subsample supported for gdbt and dart\n",
    "                    params['subsample'] = random.sample(SUBSAMPLE_DIST, 1)[0]\n",
    "                \n",
    "            results_list = self._random_objective(params, i)\n",
    "            random_results = pd.DataFrame(columns = ['loss', 'params', 'iteration', 'estimators', 'time'],\n",
    "                                       index = list(range(MAX_EVALS)))\n",
    "            # Add results to next row in dataframe\n",
    "            random_results.loc[i, :] = results_list\n",
    "            # Sort results by best validation score\n",
    "            random_results.sort_values('loss', ascending = True, inplace = True)\n",
    "            random_results.reset_index(inplace = True, drop = True)\n",
    "            \n",
    "            best_random_params = random_results.loc[0, 'params'].copy()\n",
    "            best_random_estimators = int(random_results.loc[0, 'estimators'])\n",
    "            print(best_random_params)\n",
    "            self._model = lgb.LGBMClassifier(n_estimators=best_random_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', **best_random_params, random_state = 50)\n",
    "            print('This was achieved using {} search iterations.'.format(random_results.loc[0, 'iteration']))\n",
    "    \n",
    "    def fit_model(self):\n",
    "        start = timer()\n",
    "        self._fitted_model = self._model.fit(self._features, self.labels)\n",
    "        lgb_time = timer()-start       \n",
    "        print(\"The best model from {} search training time is {:.4f} seconds\".format(self.search_type ,lgb_time))\n",
    "#         return self._fitted_model\n",
    "    \n",
    "    def transform_model(self):\n",
    "        self._predictions = self._model.predict_proba(self._test_features)[:, 1]\n",
    "        print('The best model from {} search scores {:.4f} on the test data.'.format(self.search_type, roc_auc_score(self.test_labels, self._predictions)))\n",
    "#         return self._prediction\n",
    "            \n",
    "    def evaluate_result(self):\n",
    "        auc = roc_auc_score(self.test_labels, self._predictions)\n",
    "        print(\"The AUC from {} search is {:.4f}\".format(self.search_type, auc))\n",
    "#         return auc\n",
    "    \n",
    "    \n",
    "    def _random_objective(self, params, iteration, n_folds = N_FOLDS):\n",
    "        \"\"\"Random search objective function. Takes in hyperparameters\n",
    "           and returns a list of results to be saved.\"\"\"\n",
    "        start = timer()\n",
    "        # Perform n_folds cross validation\n",
    "        if params['boosting_type'] == 'dart': \n",
    "            cv_results = lgb.cv(params, self._train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                            metrics = 'auc', seed = 50)\n",
    "        else:\n",
    "            cv_results = lgb.cv(params, self._train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                            early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "        end = timer()\n",
    "        best_score = np.max(cv_results['auc-mean'])\n",
    "        # Loss must be minimized\n",
    "        loss = 1 - best_score\n",
    "        # Boosting rounds that returned the highest cv score\n",
    "        n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "        # Return list of results\n",
    "        return [loss, params, iteration, n_estimators, end - start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (5822, 85)\n",
      "Test shape:  (4000, 85)\n",
      "Successfully load training data!\n",
      "The best model from default search training time is 0.1311 seconds\n",
      "The best model from default search scores 0.7092 on the test data.\n",
      "The AUC from default search is 0.7092\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lgb_model = TrainLGBModel(search_type = 'default')\n",
    "    lgb_model.execute_pipeline(data = 'caravan-insurance-challenge.csv', label = 'CARAVAN', train_test_col = 'ORIGIN')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
