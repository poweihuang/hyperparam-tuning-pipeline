{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class ModelPipeline(object):\n",
    "    \n",
    "    __metaclass__ = abc.ABCMeta\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_data(self, data, label, train_test_col):\n",
    "        pass \n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def optimize_hyperparam(self):\n",
    "        pass    \n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def fit_model(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def transform_model(self):\n",
    "        pass\n",
    "    # Hoook me\n",
    "    def ensemble_model(self):\n",
    "        pass    \n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def evaluate_result(self):\n",
    "        pass\n",
    "    \n",
    "    #定義method的執行流程\n",
    "    def execute_pipeline(self, data, label, train_test_col):\n",
    "        self.get_data(data, label, train_test_col)\n",
    "        self.optimize_hyperparam()\n",
    "        self.fit_model()\n",
    "        self.transform_model()\n",
    "        self.evaluate_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import logging\n",
    "import random\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# https://github.com/hyperopt/hyperopt\n",
    "from hyperopt import STATUS_OK, hp, tpe, Trials, fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "MAX_EVALS = 10\n",
    "N_FOLDS = 5\n",
    "# lgb hyperparameter grid\n",
    "PARAM_GRID = {\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(30, 150)),\n",
    "    'learning_rate': list(np.logspace(np.log(0.005), np.log(0.2), base = np.exp(1), num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10))\n",
    "}\n",
    "# Subsampling (only applicable with 'goss')\n",
    "SUBSAMPLE_DIST = list(np.linspace(0.5, 1, 100))\n",
    "\n",
    " # boosting type domain \n",
    "BOOSTING_TYPE = {'boosting_type': hp.choice('boosting_type', \n",
    "                                            [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)}, \n",
    "#                                              {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                                             {'boosting_type': 'goss', 'subsample': 1.0}])}\n",
    "\n",
    "SPACE = {\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
    "#                                                  {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}\n",
    "\n",
    "\n",
    "class TrainLGBModel(ModelPipeline):\n",
    "    def __init__(self, search_type):\n",
    "        self.search_type = search_type\n",
    "        self.model = None\n",
    "        self.fitted_model = None\n",
    "        self.prediction = None\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "        self.test_labels = None\n",
    "        self.test_features = None\n",
    "        self.train_set = None\n",
    "    \n",
    "    def get_data(self, data, label, train_test_col):\n",
    "        data = pd.read_csv(data)\n",
    "        train = data[data[train_test_col] == 'train']\n",
    "        test = data[data[train_test_col] == 'test']\n",
    "        \n",
    "        train_labels = np.array(train[label].astype(np.int32)).reshape((-1,))\n",
    "        self.test_labels = np.array(test[label].astype(np.int32)).reshape((-1,))\n",
    "        \n",
    "        train = train.drop(columns = [train_test_col, label])\n",
    "        test = test.drop(columns = [train_test_col, label])\n",
    "        self.features = np.array(train)\n",
    "        self.test_features = np.array(test)\n",
    "        self.labels = train_labels[:] \n",
    "        print('Train shape: ', train.shape)\n",
    "        print('Test shape: ', test.shape)\n",
    "        print(\"Successfully load training data!\")\n",
    "        self.train_set = lgb.Dataset(self.features, label = self.labels)\n",
    "    \n",
    "    def optimize_hyperparam(self):\n",
    "        if self.search_type == 'default':\n",
    "            self.model = lgb.LGBMClassifier()\n",
    "        if self.search_type == 'bayesian':\n",
    "            global  ITERATION\n",
    "            ITERATION = 0            \n",
    "            # Draw a sample\n",
    "            params = sample(BOOSTING_TYPE)\n",
    "            print(params)\n",
    "            # Sample from the full space\n",
    "            x = sample(SPACE)\n",
    "            print(x)\n",
    "            # Conditional logic to assign top-level keys\n",
    "            subsample = x['boosting_type'].get('subsample', 1.0)\n",
    "            x['boosting_type'] = x['boosting_type']['boosting_type']\n",
    "            x['subsample'] = subsample\n",
    "            print(x) \n",
    "            # optimization algorithm\n",
    "            tpe_algorithm = tpe.suggest\n",
    "            # Keep track of results\n",
    "            bayes_trials = Trials()\n",
    "            # File to save first results\n",
    "            self.out_file = 'results/gbm_trials.csv'\n",
    "            of_connection = open(self.out_file, 'w')\n",
    "            writer = csv.writer(of_connection)\n",
    "\n",
    "            # Write the headers to the file\n",
    "            writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
    "            of_connection.close()\n",
    "            \n",
    "            # Global variable\n",
    "            # Run optimization\n",
    "            best = fmin(fn = self._gradient_boosting_objective, space = SPACE, algo = tpe.suggest, \n",
    "                        max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(50))\n",
    "            # Sort the trials with lowest loss (highest AUC) first\n",
    "            bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\n",
    "            bayes_trials_results[:2]\n",
    "            \n",
    "            results = pd.read_csv('results/gbm_trials.csv')\n",
    "\n",
    "            # Sort with best scores on top and reset index for slicing\n",
    "            results.sort_values('loss', ascending = True, inplace = True)\n",
    "            results.reset_index(inplace = True, drop = True)\n",
    "            results.head()\n",
    "            \n",
    "            # Convert from a string to a dictionary\n",
    "            ast.literal_eval(results.loc[0, 'params'])\n",
    "              \n",
    "            # Extract the ideal number of estimators and hyperparameters\n",
    "            best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "            best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "            # Re-create the best model and train on the training data\n",
    "            self.model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1, \n",
    "                                                   objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "            print('This was achieved after {} search iterations'.format(results.loc[0, 'iteration']))\n",
    "\n",
    "        if self.search_type == 'random':\n",
    "            random.seed(100)\n",
    "            for i in range(MAX_EVALS):\n",
    "                # Randomly sample parameters for gbm\n",
    "                params = {key: random.sample(value, 1)[0] for key, value in PARAM_GRID.items()}            \n",
    "                if params['boosting_type'] == 'goss':\n",
    "                    # Cannot subsample with goss\n",
    "                    params['subsample'] = 1.0                    \n",
    "                else:\n",
    "                    # Subsample supported for gdbt and dart\n",
    "                    params['subsample'] = random.sample(SUBSAMPLE_DIST, 1)[0]\n",
    "                \n",
    "            results_list = self._random_objective(params, i)\n",
    "            random_results = pd.DataFrame(columns = ['loss', 'params', 'iteration', 'estimators', 'time'],\n",
    "                                       index = list(range(MAX_EVALS)))\n",
    "            # Add results to next row in dataframe\n",
    "            random_results.loc[i, :] = results_list\n",
    "            # Sort results by best validation score\n",
    "            random_results.sort_values('loss', ascending = True, inplace = True)\n",
    "            random_results.reset_index(inplace = True, drop = True)\n",
    "            \n",
    "            best_random_params = random_results.loc[0, 'params'].copy()\n",
    "            best_random_estimators = int(random_results.loc[0, 'estimators'])\n",
    "            print(best_random_params)\n",
    "            self.model = lgb.LGBMClassifier(n_estimators=best_random_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', **best_random_params, random_state = 50)\n",
    "            print('This was achieved using {} search iterations.'.format(random_results.loc[0, 'iteration']))\n",
    "    \n",
    "    def fit_model(self):\n",
    "        start = timer()\n",
    "        self.fitted_model = self.model.fit(self.features, self.labels)\n",
    "        lgb_time = timer()-start       \n",
    "        print(\"The best model from {} search training time is {:.4f} seconds\".format(self.search_type ,lgb_time))\n",
    "    \n",
    "    def transform_model(self):\n",
    "        self.predictions = self.model.predict_proba(self.test_features)[:, 1]\n",
    "        print('The best model from {} search scores {:.4f} on the test data.'.format(self.search_type, roc_auc_score(self.test_labels, self.predictions)))\n",
    "            \n",
    "    def evaluate_result(self):\n",
    "        auc = roc_auc_score(self.test_labels, self.predictions)\n",
    "        print(\"The AUC from {} search is {:.4f}\".format(self.search_type, auc))\n",
    "    \n",
    "    \n",
    "    def _random_objective(self, params, iteration, n_folds = N_FOLDS):\n",
    "        \"\"\"Random search objective function. Takes in hyperparameters\n",
    "           and returns a list of results to be saved.\"\"\"\n",
    "        start = timer()\n",
    "        # Perform n_folds cross validation\n",
    "        if params['boosting_type'] == 'dart': \n",
    "            cv_results = lgb.cv(params, self.train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                            metrics = 'auc', seed = 50)\n",
    "        else:\n",
    "            cv_results = lgb.cv(params, self.train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                            early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n",
    "        end = timer()\n",
    "        best_score = np.max(cv_results['auc-mean'])\n",
    "        # Loss must be minimized\n",
    "        loss = 1 - best_score\n",
    "        # Boosting rounds that returned the highest cv score\n",
    "        n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "        # Return list of results\n",
    "        return [loss, params, iteration, n_estimators, end - start]\n",
    "    \n",
    "    def _gradient_boosting_objective(self, params, n_folds = N_FOLDS):\n",
    "        \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION += 1\n",
    "        # Retrieve the subsample if present otherwise set to 1.0\n",
    "        subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "        # Extract the boosting type\n",
    "        params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "        params['subsample'] = subsample\n",
    "        # Make sure parameters that need to be integers are integers\n",
    "        for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "            params[parameter_name] = int(params[parameter_name])\n",
    "        start = timer()\n",
    "        # Perform n_folds cross validation\n",
    "        if params['boosting_type'] == 'dart': \n",
    "            cv_results = lgb.cv(params, self.train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                            metrics = 'auc', seed = 50)\n",
    "        else:\n",
    "            cv_results = lgb.cv(params, self.train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                            early_stopping_rounds = 100, metrics = 'auc', seed = 50)        \n",
    "        run_time = timer() - start\n",
    "        # Extract the best score\n",
    "        best_score = np.max(cv_results['auc-mean'])\n",
    "        # Loss must be minimized\n",
    "        loss = 1 - best_score\n",
    "        # Boosting rounds that returned the highest cv score\n",
    "        n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "        # Write to the csv file ('a' means append)\n",
    "        of_connection = open(self.out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow([loss, params, ITERATION, \n",
    "                         n_estimators, run_time])\n",
    "        # Dictionary with information for evaluation\n",
    "        return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "                'estimators': n_estimators, \n",
    "                'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (5822, 85)\n",
      "Test shape:  (4000, 85)\n",
      "Successfully load training data!\n",
      "{'boosting_type': {'boosting_type': 'gbdt', 'subsample': 0.6040735905270593}}\n",
      "{'boosting_type': {'boosting_type': 'gbdt', 'subsample': 0.9560323200138281}, 'class_weight': None, 'colsample_bytree': 0.8310823192173599, 'learning_rate': 0.029642399954777503, 'min_child_samples': 260.0, 'num_leaves': 43.0, 'reg_alpha': 0.7654396659123995, 'reg_lambda': 0.6753631548946817, 'subsample_for_bin': 120000.0}\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.8310823192173599, 'learning_rate': 0.029642399954777503, 'min_child_samples': 260.0, 'num_leaves': 43.0, 'reg_alpha': 0.7654396659123995, 'reg_lambda': 0.6753631548946817, 'subsample_for_bin': 120000.0, 'subsample': 0.9560323200138281}\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.01it/s, best loss: 0.23465460856485376]\n",
      "This was achieved after 5 search iterations\n",
      "The best model from bayesian search training time is 0.2890 seconds\n",
      "The best model from bayesian search scores 0.7283 on the test data.\n",
      "The AUC from bayesian search is 0.7283\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    lgb_model = TrainLGBModel(search_type = 'bayesian')\n",
    "    lgb_model.execute_pipeline(data = 'caravan-insurance-challenge.csv', label = 'CARAVAN', train_test_col = 'ORIGIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
